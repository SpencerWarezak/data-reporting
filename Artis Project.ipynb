{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f25bf24",
   "metadata": {},
   "source": [
    "# Data Reporting Project\n",
    "### By Spencer Warezak -- August 30, 2023\n",
    "\n",
    "This project is for the analysis of 4 separate datasets (mdata_2023-03-10, mdata_2023-03-11, mdata_2023-03-12, mdata_2023-03-13). The object of this project is to monitor datasets for data quality and present the result in an email (with colors). The daily report for each dataset should include:\n",
    "* Duplicate data\n",
    "* Missing data\n",
    "* Bad data\n",
    "* Anomalies within the data\n",
    "* Ensuring that all 6 customers have data\n",
    "\n",
    "The data has 4 fields:\n",
    "* Time (epoch format)\n",
    "* Consumption (in kwh)\n",
    "* ChannelId\n",
    "* Whether the time is in UTC format\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 947,
   "id": "d98682ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: yagmail in /Users/spencerwarezak/opt/anaconda3/lib/python3.8/site-packages (0.15.293)\n",
      "Requirement already satisfied: premailer in /Users/spencerwarezak/opt/anaconda3/lib/python3.8/site-packages (from yagmail) (3.10.0)\n",
      "Requirement already satisfied: cssselect in /Users/spencerwarezak/opt/anaconda3/lib/python3.8/site-packages (from premailer->yagmail) (1.2.0)\n",
      "Requirement already satisfied: cachetools in /Users/spencerwarezak/opt/anaconda3/lib/python3.8/site-packages (from premailer->yagmail) (5.3.1)\n",
      "Requirement already satisfied: cssutils in /Users/spencerwarezak/opt/anaconda3/lib/python3.8/site-packages (from premailer->yagmail) (2.7.1)\n",
      "Requirement already satisfied: requests in /Users/spencerwarezak/opt/anaconda3/lib/python3.8/site-packages (from premailer->yagmail) (2.31.0)\n",
      "Requirement already satisfied: lxml in /Users/spencerwarezak/opt/anaconda3/lib/python3.8/site-packages (from premailer->yagmail) (4.6.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/spencerwarezak/opt/anaconda3/lib/python3.8/site-packages (from requests->premailer->yagmail) (2.1.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/spencerwarezak/opt/anaconda3/lib/python3.8/site-packages (from requests->premailer->yagmail) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/spencerwarezak/opt/anaconda3/lib/python3.8/site-packages (from requests->premailer->yagmail) (2020.12.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/spencerwarezak/opt/anaconda3/lib/python3.8/site-packages (from requests->premailer->yagmail) (2.10)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install yagmail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 956,
   "id": "f2d30e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import pandas library for data consumption and validation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import yagmail\n",
    "\n",
    "\n",
    "## define our input files globally\n",
    "input_strings = [\n",
    "    \"{}/{}\".format(os.getcwd(),'./mdata-2023-03-10.csv'),\n",
    "    \"{}/{}\".format(os.getcwd(),'./mdata-2023-03-11.csv'),\n",
    "    \"{}/{}\".format(os.getcwd(),'./mdata-2023-03-12.csv'),\n",
    "    \"{}/{}\".format(os.getcwd(),'./mdata-2023-03-13.csv')\n",
    "]\n",
    "\n",
    "## define a new object class to handle all of the reporting data\n",
    "## Our report class will be populated to handle all of our value tracking\n",
    "## and will be used for the purpose of report generation\n",
    "\n",
    "## The following fields will be used in our report\n",
    "## INITIALIZED VALUE(S)\n",
    "## * df: This is our dataframe being used for the report\n",
    "\n",
    "## INTERNAL VALUE(S)\n",
    "## * duplicates_count: This is the count for number of times we see a row\n",
    "## * duplicates_list: This is the list mapping row string to the row object\n",
    "## * missing: This is the list of missing rows based on epoch intervals\n",
    "## * bad: This is the number of bad values we have (NaN, inf, -inf, poor formatting, etc.)\n",
    "## * outliers: This is a list of row values with outlier consumption values (+/- 3 std deviations from the mean)\n",
    "## * channel_ids: The list of unique channel ids we see (this will be used to validate that we have all 6 customers)\n",
    "\n",
    "## TRACKING VALUE(S)\n",
    "## * interval: This is the interval calculated between epochs for each customer\n",
    "## * mean: This is the mean consumption value for each customer in the dataset\n",
    "## * stddev: This is the stddev of consumption values for each customer in the dataset\n",
    "class Report():\n",
    "    ## GLOBAL VARIABLES\n",
    "    ## Colors and font styling\n",
    "    PURPLE = '<span style=\"color: purple;\">'\n",
    "    GREEN = '<span style=\"color: green;\">'\n",
    "    ORANGE = '<span style=\"color: orange;\">'\n",
    "    RED = '<span style=\"color: red;\">'\n",
    "    RESET = '</span>'\n",
    "    BOLD = '<span style=\"font-weight: bold;\">'\n",
    "    ITALIC = '<span style=\"font-style: italic;\">'\n",
    "    \n",
    "    ## INIT\n",
    "    ## The initialization of the Report should handle all of the data parsing\n",
    "    ## It will call all internal methods\n",
    "    ## There should be no data processing required after initialization\n",
    "    ## It should populate the duplicates, missing, bad, outliers, and channel ids\n",
    "    ## \n",
    "    ## Initially, the __init__ function will set the interval, mean, and stddev for each channel id\n",
    "    def __init__(self, df, title) -> None:\n",
    "        ## Init val(s)\n",
    "        self.__title = title\n",
    "        self.__df = df\n",
    "        \n",
    "        ## Internal val(s)\n",
    "        self.__duplicates_count = {}\n",
    "        self.__duplicates_list = {}\n",
    "        self.__missing = {}\n",
    "        self.__bad = {}\n",
    "        self.__outliers = {}\n",
    "\n",
    "        ## Tracking val(s)\n",
    "        self.__interval = {}\n",
    "        self.__mean = {}\n",
    "        self.__stddev = {}\n",
    "        \n",
    "        ## Set channel ids to be non na and non null values in channel id\n",
    "        ## Remove all null channelid rows\n",
    "        self.__channel_ids = [id for id in self.__df.channelid.unique() if not pd.isna(id)]\n",
    "        self.__df = self.__df[self.__df['channelid'].isin(self.__channel_ids)]\n",
    "        \n",
    "        ## Set the duplicates\n",
    "        ## True/False values for duplicates ==> all column values for a row occur more than once\n",
    "        duplicate_mask = self.__df.duplicated(subset=['epoch', 'data', 'channelid', 'tz'], keep=False)\n",
    "        for index, row in self.__df[duplicate_mask].iterrows():\n",
    "            cid = row.channelid\n",
    "            if cid not in self.__duplicates_count:\n",
    "                self.__duplicates_count[cid] = 0\n",
    "            self.__duplicates_count[cid] += 1\n",
    "            \n",
    "            if cid not in self.__duplicates_list:\n",
    "                self.__duplicates_list[cid] = []\n",
    "            if row.to_dict() not in self.__duplicates_list[cid]:\n",
    "                self.__duplicates_list[cid].append(row.to_dict())\n",
    "                            \n",
    "        ## Drop all duplicated values from the dataset\n",
    "        self.__df = self.__df.drop_duplicates(subset=['epoch', 'data', 'channelid', 'tz'])\n",
    "                \n",
    "        ## Set the bad values and fix/remove\n",
    "        ## Group by channelid and \n",
    "        ##     Generate frequency map for interval\n",
    "        ##     Find all na values and handle via interpolation\n",
    "        ##\n",
    "        rows_to_drop = []\n",
    "        grouped_df = self.__df.groupby('channelid')\n",
    "        for cid, subset in grouped_df:\n",
    "            ## Frequency map for epoch differences\n",
    "            freq_map = {}\n",
    "            for i in range(len(subset) - 1):\n",
    "                epoch_diff = abs(subset.iloc[i+1].epoch - subset.iloc[i].epoch)\n",
    "                if epoch_diff not in freq_map:\n",
    "                    freq_map[epoch_diff] = 0\n",
    "                freq_map[epoch_diff] += 1\n",
    "            \n",
    "            ## Set the interval to the most frequently occurring difference\n",
    "            self.__interval[cid] = max(freq_map, key=freq_map.get)\n",
    "            \n",
    "            ## Bad values\n",
    "            ## Sum the count of all na values and non_numeric values\n",
    "            ## For each group, denote the number of bad values\n",
    "            ## Interpolate these values\n",
    "            self.__bad[cid] = []\n",
    "            bad_mask = subset.isna().any(axis=1)\n",
    "            if bad_mask.any():\n",
    "                for index, row in subset[bad_mask].iterrows():\n",
    "                    if row.to_dict() not in self.__bad[cid]:\n",
    "                        self.__bad[cid].append(row.to_dict())\n",
    "                    rows_to_drop.append(index)\n",
    "                        \n",
    "            non_numeric_mask = pd.to_numeric(subset.data, errors='coerce').isna()\n",
    "            if non_numeric_mask.any():\n",
    "                for index, row in subset[non_numeric_mask].iterrows():\n",
    "                    if row.to_dict() not in self.__bad[cid]:\n",
    "                        self.__bad[cid].append(row.to_dict())\n",
    "                    rows_to_drop.append(index)\n",
    "            \n",
    "        ## Drop the rows we no longer need\n",
    "        ## Reset the index of the dataframe to accurately\n",
    "        ## represent its current state\n",
    "        self.__df.drop(index=rows_to_drop, inplace=True)\n",
    "        self.__df.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        ## Now that we have dropped all bad values and removed all duplicate values\n",
    "        ## we can set column data type\n",
    "        self.__df.data.astype(float)\n",
    "        \n",
    "        ## Iterate through all the rows of each group and find the missing values\n",
    "        ## A missing value is defined as not being recorded within the interval stated\n",
    "        ## Rows will be added in the form { epoch: curr + interval, data: np.nan, channelid: cid, tz: 'UTZ' }\n",
    "        ## The na values will then be interpolated\n",
    "        updated_df = self.__df.copy()\n",
    "        for i in range(len(self.__df) - 1):\n",
    "            next, curr = self.__df.iloc[i+1], self.__df.iloc[i]\n",
    "            epoch_diff = abs(next.epoch - curr.epoch)\n",
    "            \n",
    "            if next.channelid == curr.channelid:\n",
    "                cid = curr.channelid\n",
    "                if self.__interval[cid] < epoch_diff:\n",
    "                    ## Check to see if the channel id is in the missing map\n",
    "                    if cid not in self.__missing:\n",
    "                        self.__missing[cid] = []\n",
    "                    \n",
    "                    curr = epoch_diff - self.__interval[cid]\n",
    "                    rows = []\n",
    "                    while curr > 0:\n",
    "                        ## Append the data to the current rows list\n",
    "                        ## Append the data to the missing data map\n",
    "                        data = { 'epoch': next.epoch + curr, 'data': np.nan, 'channelid': cid, 'tz': 'UTC' }\n",
    "                        rows.append(data)\n",
    "                        self.__missing[cid].append(data)\n",
    "                        \n",
    "                        ## Decrement curr to the next interval\n",
    "                        curr -= self.__interval[cid]\n",
    "                        \n",
    "                    ## Modify the updated dataframe with the rows we have added\n",
    "                    ## Ensure it is in the correct location, not overwriting data\n",
    "                    updated_df = pd.concat(\n",
    "                        [updated_df.loc[:i],\n",
    "                        pd.DataFrame(rows),\n",
    "                         updated_df.loc[i+len(rows):]]\n",
    "                    ).reset_index(drop=True)\n",
    "\n",
    "        ## Set the dataframe to our updated dataframe\n",
    "        ## Interpolate missing values\n",
    "        self.__df = updated_df\n",
    "        self.__df.data = pd.to_numeric(self.__df.data, errors='coerce')\n",
    "        self.__df.data = self.__df.data.interpolate()\n",
    "        \n",
    "        grouped_df = self.__df.groupby('channelid')\n",
    "        for cid, data in grouped_df:\n",
    "            self.__mean[cid] = data.data.mean()\n",
    "            self.__stddev[cid] = data.data.std()\n",
    "            \n",
    "        ## Set the outliers based on value being >= 3 std deviations from the mean\n",
    "        for i in range(len(self.__df)):\n",
    "            curr = self.__df.iloc[i]\n",
    "            if curr.channelid not in self.__outliers:\n",
    "                self.__outliers[curr.channelid] = []\n",
    "            \n",
    "            ## Check if diff between mean and data val is >= 3 std devs\n",
    "            mean_diff = abs(curr.data - self.__mean[curr.channelid])\n",
    "            if mean_diff // self.__stddev[curr.channelid] >= 3:\n",
    "                self.__outliers[curr.channelid].append(curr.to_dict())\n",
    "                \n",
    "    ## This method will send the email report to the intended user with the report string\n",
    "    ## It will take the below parameters\n",
    "    ##     1) from email\n",
    "    ##     2) from password\n",
    "    ##     3) to email\n",
    "    def email_report(self, from_email, from_pwd, to_email) -> str:\n",
    "        if not from_email or not to_email:\n",
    "            return 'Please define a sender and receiver!'\n",
    "\n",
    "        try:\n",
    "            yag = yagmail.SMTP(from_email, from_pwd)\n",
    "            subject = f\"Data Quality Report for {self.__title}\"\n",
    "            message = self.generate_report()\n",
    "            csv_string = self.__df.to_csv('report.csv', index=False)\n",
    "            attachments=[\"{}/{}\".format(os.getcwd(),'./report.csv')]\n",
    "            \n",
    "            yag.send(to=to_email, subject=subject, contents=message, attachments=attachments)\n",
    "            return f\"Email successfully sent to {to_email}!\"\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "                                            \n",
    "    ## This method will generate a report that we can send via email\n",
    "    ## It will denote the below documented functionality:\n",
    "    ##     1) Duplicate data\n",
    "    ##     2) Missing data\n",
    "    ##     3) Bad data\n",
    "    ##     4) Anomalies within the data\n",
    "    ##     5) Ensuring that all 6 customers have data\n",
    "    ## \n",
    "    ## This string should be colored with the given color scheme\n",
    "    ##     * Green = good\n",
    "    ##     * ORANGE = passable\n",
    "    ##     * Red = poor\n",
    "    def generate_report(self) -> str: \n",
    "        ## Customer total data values\n",
    "        ## This is important to have, as we will\n",
    "        ## be assigning data quality statuses based\n",
    "        ## on the frequency of a certain category for\n",
    "        ## each customer relative to the total data\n",
    "        ## shown for them\n",
    "        customer_totals = {}\n",
    "        grouped_df = self.__df.groupby('channelid')\n",
    "        for cid, df in grouped_df:\n",
    "            customer_totals[cid] = len(df)\n",
    "                \n",
    "        ## Data availability\n",
    "        data_availability = self.GREEN if len(self.__channel_ids) == 6 else self.RED\n",
    "        \n",
    "        ## Duplicate data\n",
    "        ## Calculate duplicates as a percentage of total observations for each customer\n",
    "        duplicates_table = f\"\"\"\\n\"\"\"\n",
    "        data_duplicates = f\"\"\"[ \"\"\"\n",
    "        for cid in self.__channel_ids:\n",
    "            count_val = 0\n",
    "            if cid in self.__duplicates_count:\n",
    "                count_val = self.__duplicates_count[cid]\n",
    "                duplicates_table += f\"\"\"\\n\\t----- {cid} -----\\n{self.PURPLE}{pd.DataFrame(self.__duplicates_list[cid])}{self.RESET}\\n\\n\"\"\"\n",
    "                \n",
    "                \n",
    "            data_duplicates += f\"{cid}: {self.get_status(count_val, customer_totals[cid])}{count_val}/{customer_totals[cid]}{self.RESET}  \"\n",
    "            \n",
    "        data_duplicates += \"]\\n\"\n",
    "        \n",
    "        if duplicates_table == '\\n':\n",
    "            duplicates_table = f\"\"\"{self.PURPLE}No duplicates found!{self.RESET}\"\"\"\n",
    "        \n",
    "        \n",
    "        ## Bad data\n",
    "        data_bad = f\"\"\"[ \"\"\"\n",
    "        bad_table = f\"\"\"\\n\"\"\"\n",
    "        for cid in self.__channel_ids:\n",
    "            count_val = 0\n",
    "            if cid in self.__bad:\n",
    "                count_val = len(self.__bad[cid])\n",
    "                if len(self.__bad[cid]) > 0:\n",
    "                    bad_table += f\"\"\"\\n\\n----- {cid} -----\\n{self.PURPLE}{pd.DataFrame(self.__bad[cid])}{self.RESET}\\n\\n\"\"\"\n",
    "        \n",
    "            data_bad += f\"{cid}: {self.get_status(count_val, customer_totals[cid])}{count_val}/{customer_totals[cid]}{self.RESET}  \"\n",
    "            \n",
    "        data_bad += \"]\\n\"\n",
    "        \n",
    "        if bad_table == '\\n':\n",
    "            bad_table = f\"\"\"{self.PURPLE}No bad data found!{self.RESET}\"\"\"\n",
    "        \n",
    "        ## Missing data\n",
    "        data_missing = f\"\"\"[ \"\"\"\n",
    "        missing_table = f\"\"\"\\n\"\"\"\n",
    "        for cid in self.__channel_ids:\n",
    "            count_val = 0\n",
    "            if cid in self.__missing:\n",
    "                count_val = len(self.__missing[cid])\n",
    "                if len(self.__missing[cid]) > 0:\n",
    "                    missing_table += f\"\"\"\\n\\n----- {cid} -----\\n{self.PURPLE}{pd.DataFrame(self.__missing[cid])}{self.RESET}\\n\\n\"\"\"\n",
    "        \n",
    "            data_missing += f\"{cid}: {self.get_status(count_val, customer_totals[cid])}{count_val}/{customer_totals[cid]}{self.RESET}  \"\n",
    "            \n",
    "        data_missing += \"]\\n\"\n",
    "        \n",
    "        if missing_table == '\\n':\n",
    "            missing_table = f\"\"\"{self.PURPLE}No missing data found!{self.RESET}\"\"\"\n",
    "            \n",
    "            \n",
    "        ## Anomalies\n",
    "        data_anomalies = f\"\"\"[ \"\"\"\n",
    "        anomalies_table = f\"\"\"\\n\"\"\"\n",
    "        for cid in self.__outliers:\n",
    "            count_val = 0\n",
    "            if cid in self.__outliers:\n",
    "                count_val = len(self.__outliers[cid])\n",
    "                if len(self.__outliers[cid]) > 0:\n",
    "                    anomalies_table += f\"\"\"\\n\\n----- {cid} -----\\n{self.PURPLE}{pd.DataFrame(self.__outliers[cid])}{self.RESET}\\n\\n\"\"\"\n",
    "                    \n",
    "            data_anomalies += f\"{cid}: {self.get_status(count_val, customer_totals[cid])}{count_val}/{customer_totals[cid]}{self.RESET}  \"\n",
    "            \n",
    "        data_anomalies += \"]\\n\"\n",
    "        \n",
    "        if anomalies_table == '\\n':\n",
    "            anomalies_table = f\"\"\"{self.PURPLE}No anomalies found in the data set!{self.RESET}\"\"\"\n",
    "        \n",
    "        report_template = f\"\"\"\n",
    "            {self.BOLD}{self.__title} DATA QUALITY REPORT{self.RESET}\n",
    "            --------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "            {self.BOLD}LEGEND:{self.RESET} \n",
    "                - Quality: [ {self.GREEN}GOOD{self.RESET} -- {self.ORANGE}PASSABLE{self.RESET} -- {self.RED}POOR{self.RESET} -- {self.PURPLE}IMPORTANT{self.RESET} ]\n",
    "            \n",
    "            {self.BOLD}Report Overview:{self.RESET}\n",
    "                - This report summarizes the quality of the data received highlighting key\n",
    "                  aspects regarding data quality, integrity, and cleanliness. In reading the\n",
    "                  input dataset, we measured for duplicate data, missing data, bad (poorly)\n",
    "                  formatted data, outliers (consumption of >= 3 standard deviations from\n",
    "                  the mean), as well as ensuring all 6 customers have data available to them.\n",
    "                  \n",
    "            {self.BOLD}General Statistics:{self.RESET}\n",
    "                - Epoch Intervals: {self.PURPLE}{self.__interval}{self.RESET}\n",
    "                - Mean Values: {self.PURPLE}{self.__mean}{self.RESET}\n",
    "                - Standard Deviation Values: {self.PURPLE}{self.__stddev}{self.RESET}\n",
    "                \n",
    "            {self.BOLD}Data Availability:{self.RESET}\n",
    "                - Status: {data_availability}{len(self.__channel_ids)}/6{self.RESET}\n",
    "                {self.generate_data_availability_explanation()}\n",
    "                - Channel IDs Present: {self.PURPLE}{self.__channel_ids}{self.RESET}\n",
    "                \n",
    "            {self.BOLD}Duplicate Data:{self.RESET}\n",
    "                - Status: {data_duplicates}\n",
    "                {self.generate_duplicate_data_explanation()}\n",
    "                - Duplicates Tables: {duplicates_table}\n",
    "                \n",
    "            {self.BOLD}Bad Data:{self.RESET}\n",
    "                - Status: {data_bad}\n",
    "                {self.generate_bad_data_explanation()}\n",
    "                - Bad Tables: {bad_table}\n",
    "            \n",
    "            {self.BOLD}Missing Data:{self.RESET}\n",
    "                - Status: {data_missing}\n",
    "                {self.generate_missing_data_explanation()}\n",
    "                - Missing Tables: {missing_table}\n",
    "                \n",
    "            {self.BOLD}Outliers:{self.RESET}\n",
    "                - Status: {data_anomalies}\n",
    "                {self.generate_anomalies_data_explanation()}\n",
    "                - Outliers Tables: {anomalies_table}\n",
    "                \n",
    "            Data Frame Summary After Cleaning: \\n{self.PURPLE}{self.__df.data.describe()}{self.RESET}\n",
    "            \n",
    "            The cleaned data set is attached with the title '{self.ITALIC}report.csv'{self.RESET}'.\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        return report_template\n",
    "    \n",
    "    def generate_data_availability_explanation(self) -> str:\n",
    "        if len(self.__channel_ids) == 6:\n",
    "            return f\"\"\"- Explanation: Data for all 6 customers is ensured to be present. If any\n",
    "                customers are without data, we cannot provide our suite of services to\n",
    "                them. Given there are {self.GREEN}6{self.RESET} values present, we are able to service each of\n",
    "                our customers. No issues are posed in terms of data availability.\"\"\"\n",
    "        else:\n",
    "            return f\"\"\"- Explanation: Data for all 6 customers is ensured to be present. If any\n",
    "                    customers are without data, we cannot provide our suite of services to\n",
    "                    them. With {self.RED}{len(self.__channel_ids)}{self.RESET} value(s) present, we are unable to service each of our \n",
    "                    customers. This poses an issue in terms of data availability.\"\"\"\n",
    "        \n",
    "    def get_status(self, dups, total) -> str:\n",
    "        if dups / total <= 0.025:\n",
    "            return self.GREEN\n",
    "        elif dups / total <= 0.05:\n",
    "            return self.ORANGE\n",
    "        else:\n",
    "            return self.RED\n",
    "        \n",
    "    def generate_duplicate_data_explanation(self) -> str:\n",
    "        return f\"\"\"- Explanation: Duplicates found in the input dataset can alter any sort of numeric\n",
    "                accuracy we are trying to maintain. We also want to limit the number of data points we\n",
    "                are removing from our dataset. Thus, we have set the boundary for good data as having\n",
    "                duplicates less than or equal to 2.5% of all available data points, and passable data\n",
    "                as having less than or equal to 5.0% of all available data points. Anything with more\n",
    "                than 5% of all available data points being duplicates is flagged as poor data quality.\n",
    "                Thus, we arrive at the labels measured above for each customer.\n",
    "        \"\"\"\n",
    "    \n",
    "    def generate_bad_data_explanation(self) -> str:\n",
    "        return f\"\"\"- Explanation: Bad data found in the input dataset can alter any sort of numeric\n",
    "                accuracy we are trying to maintain. We also want to limit the number of data points we\n",
    "                are removing and then interpolating in our dataset. Thus, we have set the boundary for \n",
    "                good data as having bad inputs less than or equal to 2.5% of all available data points, \n",
    "                and passable data as having less than or equal to 5.0% of all available data points. \n",
    "                Anything with more than 5% of all available data points being bad is flagged as \n",
    "                poor data quality. Thus, we arrive at the labels measured above for each customer.\n",
    "        \"\"\"\n",
    "    \n",
    "    def generate_missing_data_explanation(self) -> str:\n",
    "        return f\"\"\"- Explanation: Missing data found in the input dataset can alter any sort of numeric\n",
    "                accuracy we are trying to maintain. We also want to limit the number of data points we\n",
    "                are adding and then interpolating values for in our dataset. Thus, we have set the boundary for \n",
    "                good data as having missing inputs less than or equal to 2.5% of all available data points, \n",
    "                and passable data as having less than or equal to 5.0% of all available data points. \n",
    "                Anything with more than 5% of all available data points being bad is flagged as \n",
    "                poor data quality. Thus, we arrive at the labels measured above for each customer.\n",
    "        \"\"\" \n",
    "    \n",
    "    def generate_anomalies_data_explanation(self) -> str:\n",
    "        return f\"\"\"- Explanation: Anomalies found in the input dataset can alter any sort of numeric\n",
    "                accuracy we are trying to maintain. Thus, we have set the boundary for good data as \n",
    "                having anomalies less than or equal to 2.5% of all available data points, and passable \n",
    "                data as having less than or equal to 5.0% of all available data points. Anything with \n",
    "                more than 5% of all available data points being bad is flagged as poor data quality. \n",
    "                Thus, we arrive at the labels measured above for each customer.\n",
    "        \"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 958,
   "id": "b327ff26",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please define a sender and receiver!\n",
      "Please define a sender and receiver!\n",
      "Please define a sender and receiver!\n",
      "Please define a sender and receiver!\n"
     ]
    }
   ],
   "source": [
    "from_email = ''\n",
    "from_pwd = ''\n",
    "to_email = ''\n",
    "\n",
    "for input in input_strings:\n",
    "    df = pd.read_csv(input)\n",
    "    rep = Report(df, input)\n",
    "    rc = rep.email_report(from_email, from_pwd, to_email)\n",
    "    print(rc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b297a278",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
